mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00001000_r_1_lr_decay_no/0"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00001000_r_1_lr_decay_no/0/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00001000_r_1_lr_decay_no/0         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00001000_r_1_lr_decay_no/0         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 0         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-05         --target_epsilon 3         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000100_r_1_lr_decay_no/0"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000100_r_1_lr_decay_no/0/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000100_r_1_lr_decay_no/0         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000100_r_1_lr_decay_no/0         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 0         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-06         --target_epsilon 3         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000010_r_1_lr_decay_no/0"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000010_r_1_lr_decay_no/0/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000010_r_1_lr_decay_no/0         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000010_r_1_lr_decay_no/0         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 0         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-07         --target_epsilon 3         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00001000_r_1_lr_decay_no/0"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00001000_r_1_lr_decay_no/0/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00001000_r_1_lr_decay_no/0         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00001000_r_1_lr_decay_no/0         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 0         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-05         --target_epsilon 8         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000100_r_1_lr_decay_no/0"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000100_r_1_lr_decay_no/0/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000100_r_1_lr_decay_no/0         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000100_r_1_lr_decay_no/0         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 0         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-06         --target_epsilon 8         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000010_r_1_lr_decay_no/0"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000010_r_1_lr_decay_no/0/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000010_r_1_lr_decay_no/0         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000010_r_1_lr_decay_no/0         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 0         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-07         --target_epsilon 8         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00001000_r_1_lr_decay_no/1"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00001000_r_1_lr_decay_no/1/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00001000_r_1_lr_decay_no/1         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00001000_r_1_lr_decay_no/1         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 1         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-05         --target_epsilon 3         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000100_r_1_lr_decay_no/1"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000100_r_1_lr_decay_no/1/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000100_r_1_lr_decay_no/1         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000100_r_1_lr_decay_no/1         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 1         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-06         --target_epsilon 3         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000010_r_1_lr_decay_no/1"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000010_r_1_lr_decay_no/1/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000010_r_1_lr_decay_no/1         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000010_r_1_lr_decay_no/1         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 1         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-07         --target_epsilon 3         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00001000_r_1_lr_decay_no/1"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00001000_r_1_lr_decay_no/1/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00001000_r_1_lr_decay_no/1         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00001000_r_1_lr_decay_no/1         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 1         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-05         --target_epsilon 8         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000100_r_1_lr_decay_no/1"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000100_r_1_lr_decay_no/1/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000100_r_1_lr_decay_no/1         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000100_r_1_lr_decay_no/1         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 1         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-06         --target_epsilon 8         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000010_r_1_lr_decay_no/1"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000010_r_1_lr_decay_no/1/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000010_r_1_lr_decay_no/1         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000010_r_1_lr_decay_no/1         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 1         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-07         --target_epsilon 8         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00001000_r_1_lr_decay_no/2"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00001000_r_1_lr_decay_no/2/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00001000_r_1_lr_decay_no/2         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00001000_r_1_lr_decay_no/2         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 2         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-05         --target_epsilon 3         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000100_r_1_lr_decay_no/2"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000100_r_1_lr_decay_no/2/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000100_r_1_lr_decay_no/2         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000100_r_1_lr_decay_no/2         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 2         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-06         --target_epsilon 3         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000010_r_1_lr_decay_no/2"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000010_r_1_lr_decay_no/2/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000010_r_1_lr_decay_no/2         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_3_00000000_td_0_00000010_r_1_lr_decay_no/2         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 2         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-07         --target_epsilon 3         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00001000_r_1_lr_decay_no/2"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00001000_r_1_lr_decay_no/2/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00001000_r_1_lr_decay_no/2         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00001000_r_1_lr_decay_no/2         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 2         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-05         --target_epsilon 8         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000100_r_1_lr_decay_no/2"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000100_r_1_lr_decay_no/2/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000100_r_1_lr_decay_no/2         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000100_r_1_lr_decay_no/2         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 2         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-06         --target_epsilon 8         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '


mkdir -p "/nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000010_r_1_lr_decay_no/2"
nlprun -x=john0,john1,john2,john3,john4,john5,john6,john7,john8,john9,john10,john11 -a lxuechen-private-lm-gen-release -o /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000010_r_1_lr_decay_no/2/log.out -p standard --memory 16G -t 3-0 'python -m table2text.run_language_modeling         --output_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000010_r_1_lr_decay_no/2         --task_mode e2e         --model_type gpt2         --model_name_or_path gpt2         --tokenizer_name gpt2         --per_device_train_batch_size 8         --per_device_eval_batch_size 20         --do_train         --do_eval         --line_by_line         --save_steps 500000000         --save_total_limit 1         --save_at_last no         --data_folder /nlp/scr/lxuechen/data/prefix-tuning/data/e2e_data         --logging_dir /nlp/scr/lxuechen/private-lm/date_110921/tm_e2e_mn_gpt2_np_no_tm_full_pemgn_0_10000000_nm_None_lr_0_00200000_tbs_00001024_md_00000512_psl_00000010_e_00000010_te_8_00000000_td_0_00000010_r_1_lr_decay_no/2         --logging_steps -1         --gradient_accumulation_steps 128         --learning_rate 0.002         --weight_decay 0.0         --seed 2         --evaluate_during_training "yes"         --eval_steps 100         --eval_epochs 10         --non_private no         --cache_dir /nlp/scr/lxuechen/hfcache/control/gpt2/         --max_steps -1         --max_eval_batches 5         --evaluation_strategy epoch         --evaluate_before_training no         --per_example_max_grad_norm 0.1         --max_seq_len 100         --max_generations 9223372036854775807         --max_generations_train 10         --max_generations_valid 9223372036854775807         --max_train_examples 9223372036854775807         --max_valid_examples 9223372036854775807         --max_eval_examples 9223372036854775807         --ema_model_averaging no         --ema_model_start_from 1000         --ghost_clipping yes         --target_delta 1e-07         --target_epsilon 8         --overwrite_output_dir         --lr_decay no         --num_train_epochs 10         --skip_generation no '

